{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPkQkTqhjabZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBL1nFtWjaba"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLzogEmbjabc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential \n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import dist\n",
    "from turtle import home\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from haversine import haversine\n",
    "\n",
    "\n",
    "\n",
    "def haversine_dist(x1,x2,y1,y2):\n",
    "   return haversine((x1, x2) , (y1, y2), unit='km')\n",
    "\n",
    "# Data Processing \n",
    "\n",
    "\n",
    "\n",
    "def extract_features_from_longitude_latitude(df, home_longitude, home_latitude):\n",
    "    home_coords = (home_latitude, home_longitude)\n",
    "\n",
    "    df['distance_from_home'] = np.vectorize(haversine_dist)(df['mean_latitude'],df['mean_longitude'],home_latitude,home_longitude)\n",
    "\n",
    "    df['heading_to_home'] = np.arctan2(df['mean_latitude'] - home_latitude, df['mean_longitude'] - home_longitude)\n",
    "    df['heading_to_home_sin'] = np.sin(df['heading_to_home'])\n",
    "    df['heading_to_home_cos'] = np.cos(df['heading_to_home'])\n",
    "    # resample by 10m mean\n",
    "    df['time_stamp'] = df.index\n",
    "    # calculate the speed of device\n",
    "    df['time_diff'] = df['time_stamp'].diff().dt.total_seconds() / 3600  # Convert seconds to hours\n",
    "    df['distance'] = np.vectorize(haversine_dist)(df['mean_latitude'],df['mean_longitude'],df['mean_latitude'].shift(1),df['mean_longitude'].shift(1)) #calculate haversine distance\n",
    "    # df['hours'] = (df['time_stamp'].astype(int) / 10**9) / 60*60 # convert to seconds\n",
    "    # df['time_taken'] = df['hours'] - df['hours'].shift(1) # calculate time difference\n",
    "\n",
    "    df['speed'] = df['distance']/df['time_diff'] # cal speed\n",
    "    df.loc[df['speed'] > 200, 'speed'] = 0\n",
    "    df['speed_towards_home'] = df['speed'] * df['heading_to_home_cos']\n",
    "    return df\n",
    "\n",
    "# df = df.resample('10T').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import DataFrameClient\n",
    "import pandas as pd\n",
    "db_name_zuka = 'smart_home_zukalovi'\n",
    "dataframe_client_zuka = DataFrameClient(\n",
    "            host='localhost',\n",
    "            port=8086,\n",
    "            username='root',\n",
    "            password='root',\n",
    "            database='smart_home_zukalovi',\n",
    "        )\n",
    "\n",
    "left_time_interval = pd.to_datetime('2023-10-01 00:00:00')\n",
    "right_time_interval = pd.to_datetime('2024-3-16 15:00:00')\n",
    "left_time_interval = f\"'{left_time_interval.strftime('%Y-%m-%dT%H:%M:%SZ')}'\"\n",
    "right_time_interval = f\"'{right_time_interval.strftime('%Y-%m-%dT%H:%M:%SZ')}'\"\n",
    "group_by_time_interval = '10s'\n",
    "tmp_output_water_entity_id_zuka = 'esphome_web_c771e8_ntc_temperature_b_constant'\n",
    "tmp_output_water_entity_id_2_zuka = 'esphome_web_c771e8_ntc_temperature_b_constant_2'\n",
    "tmp_boiler_case_entity_id_zuka = 'esphome_web_c771e8_tmp3'\n",
    "relay_entity_id_zuka = 'shelly1pm_84cca8b07eae'\n",
    "device_tracker_entity_zuka = 'klara_z_iphone'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_client_form = DataFrameClient(\n",
    "            host='localhost',\n",
    "            port=8086,\n",
    "            username='root',\n",
    "            password='root',\n",
    "            database='smart_home_formankovi',\n",
    "        )\n",
    "db_name_form = 'smart_home_formankovi'\n",
    "tmp_output_water_entity_id_form = 'esphome_boiler_temps_ntc_temperature_b_constant'\n",
    "tmp_output_water_entity_id_2_form = 'esphome_web_b7a7f1_ntc_temperature_b_constant'\n",
    "tmp_boiler_case_entity_id_form = 'shelly1pm_34945475a969_temperature_2'\n",
    "relay_entity_id_form = 'shelly1pm_34945475a969'\n",
    "device_tracker_entity_form = 'rmx3085'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# format datetime to YYYY-MM-DDTHH:MM:SSZ\n",
    "\n",
    "\n",
    "def get_queries(db_name, left_time_interval, right_time_interval, group_by_time_interval, tmp_output_water_entity_id, tmp_output_water_entity_id_2, tmp_boiler_case_entity_id, relay_entity_id, device_tracker_entity):\n",
    "\n",
    "    return {\n",
    "        \"water_flow\": {\n",
    "            \"sql_query\": f'SELECT mean(\"value\") AS \"water_flow_L_per_minute_mean\" FROM \"{db_name}\".\"autogen\".\"L/min\" WHERE time > {left_time_interval} AND time < {right_time_interval} GROUP BY time({group_by_time_interval}) FILL(0)',\n",
    "            \"measurement\": \"L/min\",\n",
    "        },\n",
    "        \"water_temperature\": {\n",
    "            \"sql_query\": f'SELECT mean(\"value\") AS \"water_temperature_mean\" FROM \"{db_name}\".\"autogen\".\"°C\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND (\"entity_id\"=\\'{tmp_output_water_entity_id}\\' OR \"entity_id\"=\\'{tmp_output_water_entity_id_2}\\') GROUP BY time({group_by_time_interval}) FILL(0)',\n",
    "            \"measurement\": \"°C\",\n",
    "        },\n",
    "        \"temperature\": {\n",
    "            \"sql_query\": f'SELECT mean(\"temperature\") AS \"outside_temperature_mean\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"domain\"=\\'weather\\' AND \"entity_id\"=\\'domov\\' GROUP BY time({group_by_time_interval}) FILL(null)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"humidity\": {\n",
    "            \"sql_query\": f'SELECT mean(\"humidity\") AS \"outside_humidity_mean\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"domain\"=\\'weather\\' AND \"entity_id\"=\\'domov\\' GROUP BY time({group_by_time_interval}) FILL(null)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"wind_speed\": {\n",
    "            \"sql_query\": f'SELECT mean(\"wind_speed\") AS \"outside_wind_speed_mean\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"entity_id\"=\\'domov\\' GROUP BY time({group_by_time_interval}) FILL(null)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"presence\": {\n",
    "            \"sql_query\": f'SELECT count(distinct(\"friendly_name_str\")) AS \"device_presence_distinct_count\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"domain\"=\\'device_tracker\\' AND \"state\"=\\'home\\' GROUP BY time({group_by_time_interval}) FILL(0)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"boiler_water_temperature\": {\n",
    "            \"sql_query\": f'SELECT mean(\"value\") AS \"boiler_water_temperature_mean\" FROM \"{db_name}\".\"autogen\".\"°C\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"entity_id\"=\\'{tmp_boiler_case_entity_id}\\' GROUP BY time({group_by_time_interval}) FILL(null)',\n",
    "            \"measurement\": \"°C\",\n",
    "        },\n",
    "        \"boiler_relay_status\": {\n",
    "            \"sql_query\": f'SELECT last(\"value\") AS \"boiler_relay_status\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"entity_id\"=\\'{relay_entity_id}\\' GROUP BY time({group_by_time_interval}) FILL(null)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"device_longitude\": {\n",
    "            \"sql_query\": f'SELECT mean(\"longitude\") AS \"mean_longitude\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"domain\"=\\'device_tracker\\' AND \"entity_id\"=\\'{device_tracker_entity}\\' GROUP BY time({group_by_time_interval}) FILL(linear)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \"device_latitude\": {\n",
    "            \"sql_query\": f'SELECT mean(\"latitude\") AS \"mean_latitude\" FROM \"{db_name}\".\"autogen\".\"state\" WHERE time > {left_time_interval} AND time < {right_time_interval} AND \"domain\"=\\'device_tracker\\' AND \"entity_id\"=\\'{device_tracker_entity}\\' GROUP BY time({group_by_time_interval}) FILL(linear)',\n",
    "            \"measurement\": \"state\",\n",
    "        },\n",
    "        \n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_list = []\n",
    "\n",
    "# for key, value in get_queries(db_name = db_name_form, left_time_interval = left_time_interval, right_time_interval = right_time_interval, group_by_time_interval = group_by_time_interval, tmp_output_water_entity_id = tmp_output_water_entity_id_form, tmp_output_water_entity_id_2 = tmp_output_water_entity_id_2_form, tmp_boiler_case_entity_id = tmp_boiler_case_entity_id_form, relay_entity_id = relay_entity_id_form, device_tracker_entity = device_tracker_entity_form).items():\n",
    "#     print(\"Querying: \", key, value[\"sql_query\"])\n",
    "#     # get data from influxdb\n",
    "#     result = dataframe_client_form.query(value[\"sql_query\"])[\n",
    "#         value[\"measurement\"]\n",
    "#     ]\n",
    "\n",
    "#     df = pd.DataFrame(result)\n",
    "    \n",
    "#     df_all_list.append(df)\n",
    "\n",
    "# df_concat_form = pd.concat(df_all_list, axis=1)\n",
    "# # save as pkl\n",
    "# with open('df_form_mult_16032024.pkl', 'wb') as f:\n",
    "#     df_concat_form.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_list = []\n",
    "#     # iterate over key an value in data\n",
    "# for key, value in get_queries(db_name=db_name_zuka, left_time_interval=left_time_interval, right_time_interval=right_time_interval, group_by_time_interval='10s', tmp_output_water_entity_id=tmp_output_water_entity_id_2_zuka, tmp_output_water_entity_id_2=tmp_output_water_entity_id_zuka, tmp_boiler_case_entity_id=tmp_boiler_case_entity_id_zuka, relay_entity_id=relay_entity_id_zuka, device_tracker_entity=device_tracker_entity_zuka).items():\n",
    "#     print(\"Querying: \", key, value[\"sql_query\"])\n",
    "#     # get data from influxdb\n",
    "#     result = dataframe_client_zuka.query(value[\"sql_query\"])[\n",
    "#         value[\"measurement\"]\n",
    "#     ]\n",
    "\n",
    "#     df = pd.DataFrame(result)\n",
    "    \n",
    "#     df_all_list.append(df)\n",
    "\n",
    "# df_concat_zuka = pd.concat(df_all_list, axis=1)\n",
    "# # save as pkl\n",
    "# with open('df_zuka_mult_16032024.pkl', 'wb') as f:\n",
    "#     df_concat_zuka.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df form pkl\n",
    "import pickle \n",
    "with open('df_form_mult_16032024.pkl', 'rb') as f:\n",
    "    df_concat_form = pickle.load(f)\n",
    "    df_concat_form = df_concat_form.dropna(subset=['water_temperature_mean'])\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_zuka_mult_16032024.pkl', 'rb') as f:\n",
    "    df_concat_zuka = pickle.load(f)\n",
    "    df_concat_zuka = df_concat_zuka.dropna(subset=['water_temperature_mean'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_form['household_members'] = 2\n",
    "df_concat_zuka['household_members'] = 6\n",
    "df_concat_form['boiler_size'] = 80\n",
    "df_concat_zuka['boiler_size'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#49.412897925874184, 16.514843458109933\n",
    "zuka_home_latitude = 49.412897925874184\n",
    "zuka_home_longitude = 16.514843458109933\n",
    "df_copy_zuka = df_concat_zuka.copy()\n",
    "df_extracted_zuka = extract_features_from_longitude_latitude(df_copy_zuka, home_longitude=zuka_home_longitude, home_latitude=zuka_home_latitude)\n",
    "# all value in speed larger than 200 set to 0\n",
    "df_extracted_zuka.loc[df_extracted_zuka['speed'] > 200, 'speed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49.39534649920643, 16.527887919743097\n",
    "form_home_latitude = 49.39534649920643\n",
    "form_home_longitude = 16.527887919743097\n",
    "\n",
    "df_copy_form = df_concat_form.copy()\n",
    "df_extracted_form = extract_features_from_longitude_latitude(df_copy_form, home_longitude=form_home_longitude, home_latitude=form_home_latitude)\n",
    "# all value in speed larger than 200 set to 0\n",
    "df_extracted_form.loc[df_extracted_form['speed'] > 200, 'speed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handler_test import DataHandlerTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_time_interval = pd.to_datetime(\"2023-10-01 00:00:00\")\n",
    "right_time_interval = pd.to_datetime(\"2024-3-16 15:00:00\")\n",
    "left_time_interval = f\"'{left_time_interval.strftime('%Y-%m-%dT%H:%M:%SZ')}'\"\n",
    "right_time_interval = f\"'{right_time_interval.strftime('%Y-%m-%dT%H:%M:%SZ')}'\"\n",
    "group_by_time_interval = \"10s\"\n",
    "tmp_output_water_entity_id_zuka = \"esphome_web_c771e8_ntc_temperature_b_constant\"\n",
    "tmp_output_water_entity_id_2_zuka = \"esphome_web_c771e8_ntc_temperature_b_constant_2\"\n",
    "tmp_boiler_case_entity_id_zuka = \"esphome_web_c771e8_tmp3\"\n",
    "relay_entity_id_zuka = \"shelly1pm_84cca8b07eae\"\n",
    "relay_power_entity_id_zuka = \"shelly1pm_84cca8b07eae_power\"\n",
    "device_tracker_entity_zuka = \"klara_z_iphone\"\n",
    "\n",
    "dataHandlerTest = DataHandlerTest(\n",
    "    \"localhost\", \"smart_home_zukalovi\", \"root\", \"root\", relay_entity_id_zuka, relay_power_entity_id_zuka, tmp_boiler_case_entity_id_zuka, tmp_output_water_entity_id_zuka, tmp_output_water_entity_id_2_zuka, device_tracker_entity_zuka\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_zuka_1 = df_extracted_zuka.loc['2023-10-01':'2023-10-15']\n",
    "df_extracted_zuka_2 = df_extracted_zuka.loc['2023-11-15':'2023-12-05']\n",
    "df_extracted_zuka_3 = df_extracted_zuka.loc['2023-12-29':]\n",
    "\n",
    "df_zuka_ext = pd.concat([df_extracted_zuka_1, df_extracted_zuka_2, df_extracted_zuka_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_form = dataHandlerTest.process_kWh_water_consumption(df_extracted_form)\n",
    "data_processed_zuka = dataHandlerTest.process_kWh_water_consumption(df_zuka_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_form_ml = dataHandlerTest.transform_data_for_ml(data_processed_form)\n",
    "data_processed_zuka_ml = dataHandlerTest.transform_data_for_ml(data_processed_zuka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_form = data_processed_form_ml[0]\n",
    "ml_data_zuka = data_processed_zuka_ml[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_zuka.plot(y='longtime_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7BQqFjcjab7"
   },
   "outputs": [],
   "source": [
    "df = ml_data_zuka.reset_index(drop=True)\n",
    "\n",
    "# #10T\n",
    "df_10T = pd.concat([df.iloc[7000:9000], df.iloc[12500:]])\n",
    "with open('df_10T.pkl', 'wb') as f:\n",
    "    df_10T.to_pickle(f)\n",
    "\n",
    "# 30T\n",
    "\n",
    "df_30T = pd.concat([df.iloc[2333:3100], df.iloc[4166:]])\n",
    "with open('df_30T.pkl', 'wb') as f:\n",
    "    df_30T.to_pickle(f)\n",
    "\n",
    "#60T\n",
    "df_60T = pd.concat([df.iloc[1166:1550], df.iloc[2083:]])\n",
    "with open('df_60T.pkl', 'wb') as f:\n",
    "    df_60T.to_pickle(f)\n",
    "\n",
    "df = df_60T.reset_index(drop=True).dropna()\n",
    "df_train = df.loc[:int(df.shape[0]*0.8),:]\n",
    "df_test = df.loc[int(df.shape[0]*0.8):,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y='longtime_mean')\n",
    "# clip to 0.6 max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yy1c-Q0pjab-"
   },
   "source": [
    "## Normalize the Data\n",
    "- for available scalers see: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "- becareful of outliers when scaling!\n",
    "- perform the normalization on the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "pRVR4nPpjab-",
    "outputId": "8034ff8e-fe74-473b-f067-56d6a4d07194"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "df_train_norm = df_train.copy()\n",
    "df_test_norm = df_test.copy()\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# # df_train_norm.loc[:,predicted_column:] = scaler.fit_transform(df_train.loc[:,predicted_column:])\n",
    "# # df_test_norm.loc[:,predicted_column:] = scaler.transform(df_test.loc[:,predicted_column:])\n",
    "# df_train_norm[df_train.columns] = scaler.fit_transform(df_train)\n",
    "# df_test_norm[df_train.columns] = scaler.transform(df_test)\n",
    "\n",
    "\n",
    "df_test_norm.plot(y='longtime_mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9Wi9R-njacI"
   },
   "source": [
    "## Use a data generator\n",
    "- creates batches on the fly\n",
    "- good for data with large number of rows and features\n",
    "- don't need to perform lags on dataframe\n",
    "- the function below was adapted from https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb\n",
    "- note: Keras added a TimeseriesGenerator class in version 2.1.5 \n",
    "    - https://github.com/keras-team/keras/releases/tag/2.1.5\n",
    "    - keras.preprocessing.sequence.TimeseriesGenerator\n",
    "keras.preprocessing.sequence.TimeseriesGenerator\n",
    "It yields a tuple `(samples, targets)` where `samples` is one batch of input data and \n",
    "`targets` is the corresponding array of target temperatures. It takes the following arguments:\n",
    "\n",
    "* `dataframe`: pandas dataframe with feature matrix and target column\n",
    "* `target_name`: string of column with target values\n",
    "* `lookback`: How many timesteps back should our input data go.\n",
    "* `delay`: How many timesteps in the future should our target be.\n",
    "* `min_index` and `max_index`: Indices in the `data` array that delimit which timesteps to draw from. This is useful for keeping a segment \n",
    "of the data for validation and another one for testing.\n",
    "* `shuffle`: Whether to shuffle our samples or draw them in chronological order.\n",
    "* `batch_size`: The number of samples per batch.\n",
    "* `step`: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q289g6TtjacI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3V-ssIMjacN"
   },
   "source": [
    "## create train, validation, test data generators\n",
    "- we will 20% of the df_train to get the validation generator\n",
    "    - use the min_index,max_index arguments for slices from generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing /Users/adamgrunwald/Desktop/FIT/smart_boiler/smartboiler/src/smartboiler/data_handler_test.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamgrunwald/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing /Users/adamgrunwald/Desktop/FIT/smart_boiler/smartboiler/src/smartboiler/forecast.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pytz import utc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_handler_test import DataHandlerTest\n",
    "from forecast import Forecast\n",
    "from datetime import datetime, timedelta\n",
    "start_of_data = datetime(2023, 11, 1)\n",
    "end_of_data = datetime(2023, 12, 10)\n",
    "end_of_training_data = datetime(2023, 12, 31, 0, 0, 0)\n",
    "dataHandler = DataHandlerTest(\n",
    "    \"localhost\",\n",
    "    \"smart_home_zukalovi\",\n",
    "    \"root\",\n",
    "    \"root\",\n",
    "    \"shelly1pm_84cca8b07eae\",\n",
    "    \"shelly1pm_84cca8b07eae_power\",\n",
    "    \"esphome_web_c771e8_tmp3\",\n",
    "    \"esphome_web_c771e8_ntc_temperature_b_constant\",\n",
    "    \"esphome_web_c771e8_ntc_temperature_b_constant_2\",\n",
    "    \"klara_z_iphone\",\n",
    "    49.412897925874184,\n",
    "    16.514843458109933,\n",
    "start_of_data,\n",
    ")\n",
    "\n",
    "\n",
    "# use previous 450 samples to predict next target ('pollution') samples\n",
    "\n",
    "predicted_columns = [\n",
    "    \"longtime_mean\",\n",
    "    # \"distance_from_home\",\n",
    "    # \"speed_towards_home\",\n",
    "    # \"count\",\n",
    "    # \"heading_to_home_sin\",\n",
    "    # \"heading_to_home_cos\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "forecast = Forecast(dataHandler, start_of_data=start_of_data, model_path='lstm_model_zukalovi.keras', predicted_columns=predicted_columns)\n",
    "forecast.load_model(left_time_interval=start_of_data, right_time_interval=end_of_training_data)\n",
    "\n",
    "# forecast.train_model(df_training_data=df_train_norm)\n",
    "# forecast.build_model(\n",
    "# )\n",
    "# forecast.fit_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 0 of 72\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.float64'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_30_minutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m next_steps, _ \u001b[38;5;241m=\u001b[39m \u001b[43mforecast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forecast_next_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_time_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_of_data_loader_for_forecast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_time_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_of_data_loader_for_forecast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m one_week_prediction\u001b[38;5;241m.\u001b[39mappend(next_steps)\n\u001b[1;32m     15\u001b[0m start_of_data_loader_for_forecast \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/FIT/smart_boiler/smartboiler/src/smartboiler/forecast.py:358\u001b[0m, in \u001b[0;36mForecast.get_forecast_next_steps\u001b[0;34m(self, left_time_interval, right_time_interval)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# append y_pred_inv to df_all\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# df_all.iloc[-1, :num_targets] = y_pred_inv[:num_targets]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# drop first row\u001b[39;00m\n\u001b[1;32m    356\u001b[0m df_all \u001b[38;5;241m=\u001b[39m df_all[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 358\u001b[0m forecast_future \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforecast_future\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m forecast_future \u001b[38;5;241m=\u001b[39m forecast_future\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    367\u001b[0m df_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_empty_row(df_all, current_forecast_begin_date, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/concat.py:346\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    144\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/concat.py:436\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    432\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    438\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# get the sample\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# want the highest ndim that we have, and must be non-empty\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# unless all objs are empty\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.float64'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "one_week_prediction = []\n",
    "number_of_30_minutes = 3*24\n",
    "end_train = datetime(2023,12,29, 0, 0, 0)\n",
    "start_of_data_loader_for_forecast = end_train\n",
    "end_of_data_loader_for_forecast = end_train + timedelta(hours=84)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, number_of_30_minutes):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Predicting {i} of {number_of_30_minutes}\")\n",
    "    next_steps, _ = forecast.get_forecast_next_steps(left_time_interval=start_of_data_loader_for_forecast, right_time_interval=end_of_data_loader_for_forecast)\n",
    "    one_week_prediction.append(next_steps)\n",
    "    start_of_data_loader_for_forecast += timedelta(minutes=60)\n",
    "    end_of_data_loader_for_forecast += timedelta(minutes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concacted = pd.concat(one_week_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_left_interval = end_train+ timedelta(hours=84)\n",
    "real_data_right_interval = real_data_left_interval + timedelta(days=3)\n",
    "real_data = dataHandler.get_data_for_prediction(real_data_left_interval, real_data_right_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(concacted[0].values.reshape(-1, 6), columns=['col1', 'col2', 'col3', 'col4', 'col5', 'col6'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "for x in range(1, 7):\n",
    "    plt.plot(new_df[f'col{x}'])\n",
    "# plt.plot(real_data['longtime_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df_test,_ = dataHandlerTest.get_data_for_training_model(left_time_interval=datetime(2024, 3, 1, 0, 0, 0, 0), right_time_interval=datetime(2024, 3, 16, 0, 0, 0, 0))\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_test_norm \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# df_train_norm.loc[:,predicted_column:] = scaler.fit_transform(df_train.loc[:,predicted_column:])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# df_test_norm.loc[:,predicted_column:] = scaler.transform(df_test.loc[:,predicted_column:])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_test_norm[df_test\u001b[38;5;241m.\u001b[39mcolumns] \u001b[38;5;241m=\u001b[39m forecast\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(df_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "# df_test,_ = dataHandlerTest.get_data_for_training_model(left_time_interval=datetime(2024, 3, 1, 0, 0, 0, 0), right_time_interval=datetime(2024, 3, 16, 0, 0, 0, 0))\n",
    "df_test_norm = df_test.copy()\n",
    "\n",
    "\n",
    "# df_train_norm.loc[:,predicted_column:] = scaler.fit_transform(df_train.loc[:,predicted_column:])\n",
    "# df_test_norm.loc[:,predicted_column:] = scaler.transform(df_test.loc[:,predicted_column:])\n",
    "df_test_norm[df_test.columns] = forecast.scaler.transform(df_test)\n",
    "\n",
    "test_gen = forecast.mul_generator(dataframe = df_test_norm, \n",
    "                     target_names = predicted_columns, \n",
    "                     lookback = forecast.lookback,\n",
    "                     delay = forecast.delay,\n",
    "                     min_index = 0,\n",
    "                     max_index = None,\n",
    "                     step = 1,\n",
    "                     shuffle = False,\n",
    "                     batch_size = df_test.shape[0])\n",
    "(X, y_truth) = next(test_gen)\n",
    "# X_reshaped = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "y_pred = forecast.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YifTrpIhjacP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_targets = len(predicted_columns)\n",
    "len_columns = len(df_test.columns)\n",
    "num_features = len_columns - num_targets\n",
    "\n",
    "print(num_targets, len_columns, num_features)\n",
    "\n",
    "# np.expand_dims(y_truth,axis=1).shape\n",
    "y_pred_inv = np.concatenate(\n",
    "    (y_pred, np.zeros((y_pred.shape[0], num_features))), axis=1\n",
    ")\n",
    "y_pred_inv = forecast.scaler.inverse_transform(y_pred_inv)\n",
    "\n",
    "y_truth_concat = np.concatenate(\n",
    "    (y_truth, np.zeros((y_truth.shape[0], num_features))), axis=1\n",
    ")\n",
    "y_truth_concat = forecast.scaler.inverse_transform(y_truth_concat)\n",
    "\n",
    "print(y_truth_concat.shape)\n",
    "print(y_pred_inv.shape)\n",
    "\n",
    "for i in range(num_targets):\n",
    "    y_pred_curr = y_pred_inv[4:, i]\n",
    "    y_truth_curr = y_truth_concat[4:, i]\n",
    "    plt.figure()\n",
    "    plt.plot(y_truth_curr[:], label=f'True {predicted_columns[i]}', alpha=0.5)\n",
    "    plt.plot(y_pred_curr[:], label=f'Predicted {predicted_columns[i]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x=y_pred_curr,y=y_truth_curr)\n",
    "    # mse = mean_squared_error(y_true=y_truth_curr, y_pred=y_pred_curr, squared=True)\n",
    "    # rmse = mean_squared_error(y_true=y_truth_curr, y_pred=y_pred_curr, squared=False)\n",
    "\n",
    "\n",
    "    print('R2 = ',r_value*r_value)\n",
    "    # print('mse = ',mse)\n",
    "    # print('rmse = ',rmse)\n",
    "    #print integral of y_pred_curr - y_truth_curr\n",
    "    print('integral = ',np.trapz(y_pred_curr - y_truth_curr))\n",
    "    print('integral_truth = ',np.trapz(y_truth_curr))\n",
    "    print('integral_pred = ',np.trapz(y_pred_curr))\n",
    "\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x=y_pred,y=y_truth)\n",
    "mse = mean_squared_error(y_true=y_truth, y_pred=y_pred, squared=True)\n",
    "rmse = mean_squared_error(y_true=y_truth, y_pred=y_pred, squared=False)\n",
    "\n",
    "\n",
    "print('R2 = ',r_value*r_value)\n",
    "print('mse = ',mse)\n",
    "print('rmse = ',rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytz import utc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_handler import DataHandler\n",
    "from forecast import Forecast\n",
    "from datetime import datetime, timedelta\n",
    "start_of_data = datetime(2023, 11, 1)\n",
    "end_of_data = datetime(2023, 12, 10)\n",
    "end_of_training_data = datetime(2023, 12, 31, 0, 0, 0)\n",
    "from data_handler_test import DataHandlerTest\n",
    "dataHandler = DataHandlerTest(\n",
    "    \"localhost\",\n",
    "    \"smart_home_zukalovi\",\n",
    "    \"root\",\n",
    "    \"root\",\n",
    "    \"shelly1pm_84cca8b07eae\",\n",
    "    \"shelly1pm_84cca8b07eae_power\",\n",
    "    \"esphome_web_c771e8_tmp3\",\n",
    "    \"esphome_web_c771e8_ntc_temperature_b_constant\",\n",
    "    \"esphome_web_c771e8_ntc_temperature_b_constant_2\",\n",
    "    device_tracker_entity_id=\"klara_z_iphone\",\n",
    "    home_longitude=49.412897925874184,\n",
    "home_latitude=16.514843458109933,\n",
    "start_of_data=start_of_data,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_norm to csv with header as colun name\n",
    "df_train_norm.to_csv('df_train_norm.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7shIrJr2jacZ"
   },
   "outputs": [],
   "source": [
    "# def r2_keras(y_true, y_pred):\n",
    "#     \"\"\"Coefficient of Determination \n",
    "#     \"\"\"\n",
    "#     SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "#     SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "#     return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination for multiple outputs\"\"\"\n",
    "    SS_res = K.sum(K.square(y_true - y_pred), axis=0)\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true, axis=0)), axis=0)\n",
    "    return K.mean(1 - SS_res / (SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9FLFXkvjacb"
   },
   "source": [
    "## Create LSTM model\n",
    "- to increase the modeling speed, replace the LSTM model with CuDNNLSTM and train on a GPU\n",
    "- can also use lightwieght GRU layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZknUNitjach"
   },
   "source": [
    "## Fit the Model\n",
    "- to increase the modeling speed, replace the LSTM model with CuDNNLSTM and train on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_Rt3pdsjacl"
   },
   "source": [
    "## Get model predictions on df_test\n",
    "- get (X,y_truth) by calling train_gen using next\n",
    "- set the batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('lstm_model_mul_var.keras', custom_objects={'r2_keras': r2_keras})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-6pPDV0jacq"
   },
   "outputs": [],
   "source": [
    "# append a value to all rows in longtimemean column zeroes\n",
    "\n",
    "test_gen = forecast.mul_generator(dataframe = df_test_norm, \n",
    "                      target_names = predicted_columns, \n",
    "                     lookback = lookback,\n",
    "                     delay = delay,\n",
    "                     min_index = 0,\n",
    "                     max_index = None,\n",
    "                     step = 1,\n",
    "                     shuffle = False,\n",
    "                     batch_size = df_test.shape[0])\n",
    "(X, y_truth) = next(test_gen)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "num_targets = len(predicted_columns)\n",
    "len_columns = len(df_test.columns)\n",
    "num_features = len_columns - num_targets\n",
    "\n",
    "# np.expand_dims(y_truth,axis=1).shape\n",
    "y_pred_inv = np.concatenate(\n",
    "    (y_pred, np.zeros((y_pred.shape[0], num_features))), axis=1\n",
    ")\n",
    "y_pred_inv = scaler.inverse_transform(y_pred_inv)\n",
    "\n",
    "y_truth = np.concatenate(\n",
    "    (y_truth, np.zeros((y_truth.shape[0], num_features))), axis=1\n",
    ")\n",
    "y_truth = scaler.inverse_transform(y_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_abs = np.abs(y_pred_inv) - 0.2\n",
    "for i in range(num_targets):\n",
    "    plt.figure()\n",
    "    plt.plot(y_truth[:, i], label=f'True {predicted_columns[i]}')\n",
    "    plt.plot(y_pred_abs[:, i], label=f'Predicted {predicted_columns[i]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    y_pred_curr = y_pred_abs[:48, i]\n",
    "    y_truth_curr = y_truth[:48, i]\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x=y_pred_curr,y=y_truth_curr)\n",
    "    mse = mean_squared_error(y_true=y_truth_curr, y_pred=y_pred_curr, squared=True)\n",
    "    rmse = mean_squared_error(y_true=y_truth_curr, y_pred=y_pred_curr, squared=False)\n",
    "\n",
    "\n",
    "    print('R2 = ',r_value*r_value)\n",
    "    print('mse = ',mse)\n",
    "    print('rmse = ',rmse)\n",
    "    #print integral of y_pred_curr - y_truth_curr\n",
    "    print('integral = ',np.trapz(y_pred_curr - y_truth_curr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_empty_row(df, date_time):\n",
    "    new_row_df = pd.DataFrame(\n",
    "        columns=df.columns,\n",
    "        data=[\n",
    "            [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                np.sin(2 * np.pi * date_time.weekday() / 7),\n",
    "                np.cos(2 * np.pi * date_time.weekday() / 7),\n",
    "                np.sin(2 * np.pi * date_time.hour / 24),\n",
    "                np.cos(2 * np.pi * date_time.hour / 24),\n",
    "                np.sin(2 * np.pi * date_time.minute / 60),\n",
    "                np.cos(2 * np.pi * date_time.minute / 60),\n",
    "            ]\n",
    "        ],\n",
    "    )\n",
    "    df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_forecast_next_steps(left_time_interval, right_time_interval):\n",
    "    # Define the indices for the different predictions and truths\n",
    "\n",
    "    num_targets = len(predicted_columns)\n",
    "    len_columns = len(df_test.columns)\n",
    "\n",
    "\n",
    "    forecast_future = pd.DataFrame()\n",
    "\n",
    "    df_all = dataHandlerTest.get_data_for_prediction(\n",
    "        left_time_interval=left_time_interval,\n",
    "        right_time_interval=right_time_interval,\n",
    "        predicted_columns=predicted_columns,\n",
    "    )\n",
    "    df_all = df_all.dropna()\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    forecast_future = pd.DataFrame()\n",
    "\n",
    "    current_forecast_begin_date = right_time_interval + timedelta(hours=0.5)\n",
    "    \n",
    "    df_all = add_empty_row(df_all, current_forecast_begin_date)\n",
    "    \n",
    "    current_forecast_begin_date += timedelta(hours=0.5)\n",
    "\n",
    "    # prediction for next 6 hours\n",
    "    for i in range(0, 12):\n",
    "        df_all = add_empty_row(df_all, current_forecast_begin_date)\n",
    "        current_forecast_begin_date += timedelta(hours=0.5)\n",
    "\n",
    "        df_predict_norm = df_all.copy()\n",
    "\n",
    "        df_predict_norm[df_all.columns] = scaler.transform(df_all)\n",
    "        # create predict df with values\n",
    "        predict_gen = mul_generator(\n",
    "            dataframe=df_predict_norm,\n",
    "            target_names=predicted_columns,\n",
    "            lookback=lookback,\n",
    "            delay=delay,\n",
    "            min_index=0,\n",
    "            max_index=None,\n",
    "            step=1,\n",
    "            shuffle=False,\n",
    "            batch_size=df_predict_norm.shape[0],\n",
    "        )\n",
    "\n",
    "        (X, y_truth) = next(predict_gen)\n",
    "\n",
    "        y_pred = model.predict(X, verbose=0)\n",
    "        # np.expand_dims(y_truth,axis=1).shape\n",
    "        y_pred_inv = np.concatenate(\n",
    "            (y_pred, np.zeros((y_pred.shape[0], len_columns-num_targets))), axis=1\n",
    "        )\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred_inv)\n",
    "        # get last predicted value\n",
    "        y_pred_inv = y_pred_inv[-1, :]\n",
    "\n",
    "        # append y_pred_inv to df_all\n",
    "        df_all.iloc[-2,:num_targets] = y_pred_inv[:num_targets]\n",
    "        # drop first row\n",
    "        df_all = df_all[1:]\n",
    "\n",
    "        forecast_future = pd.concat(\n",
    "            [\n",
    "                forecast_future,\n",
    "                df_all.iloc[-2][:num_targets],\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        forecast_future = forecast_future.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    return forecast_future, df_all\n",
    "\n",
    "# last 48 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_week_prediction = []\n",
    "number_of_30_minutes = 14*24*2\n",
    "end_train = datetime(2023,12,29, 0, 0, 0)\n",
    "start_of_data_loader_for_forecast = end_train\n",
    "end_of_data_loader_for_forecast = end_train + timedelta(hours=48)\n",
    "\n",
    "\n",
    "for i in range(0, number_of_30_minutes):\n",
    "    next_steps, _ = get_forecast_next_steps(left_time_interval=start_of_data_loader_for_forecast, right_time_interval=end_of_data_loader_for_forecast)\n",
    "    one_week_prediction.append(next_steps)\n",
    "    start_of_data_loader_for_forecast += timedelta(minutes=30)\n",
    "    end_of_data_loader_for_forecast += timedelta(minutes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = dataHandlerTest.get_data_for_prediction(left_time_interval=end_train+timedelta(hours=48), right_time_interval=end_train+timedelta(hours=48)+timedelta(minutes=30*number_of_30_minutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = []\n",
    "for df in one_week_prediction:\n",
    "    df_reshaped = (pd.DataFrame(np.reshape(df.values, (-1, 6)))).T\n",
    "    df_reshaped['prediction'] = [i for i in predicted_columns]\n",
    "\n",
    "    # transpose \n",
    "    list_df.append(df_reshaped)\n",
    "merged_df = pd.concat(list_df, ignore_index=True)\n",
    "merged_df['DataFrame_Order'] = [i for i in range(len(list_df)) for _ in range(6)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longtime_mean = merged_df[merged_df['prediction'] == 'longtime_mean']\n",
    "distance_from_home = merged_df[merged_df['prediction'] == 'distance_from_home']\n",
    "speed_towards_home = merged_df[merged_df['prediction'] == 'speed_towards_home']\n",
    "count = merged_df[merged_df['prediction'] == 'count']\n",
    "heading_to_home_sin = merged_df[merged_df['prediction'] == 'heading_to_home_sin']\n",
    "heading_to_home_cos = merged_df[merged_df['prediction'] == 'heading_to_home_cos']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift the columns by -i\n",
    "shifted_df = merged_df.copy()\n",
    "for j in predicted_columns:\n",
    "    for i in range(0,12):\n",
    "        shifted_df.loc[shifted_df['prediction'] == j, i] = shifted_df.loc[shifted_df['prediction'] == j, i].shift(-i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only columns from predicted columns\n",
    "data_truth = df_truth[predicted_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = shifted_df.copy()\n",
    "result_df['truth'] = 0  \n",
    "for prediction_column in predicted_columns:\n",
    "\n",
    "    for i in range(0,672):\n",
    "        result_df.loc[(result_df['prediction'] == prediction_column) & (result_df['DataFrame_Order'] == i), 'truth' ] = data_truth.loc[i, prediction_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predicted_column in predicted_columns:\n",
    "    number_of_steps = 1\n",
    "    #create a list of collors from 0 to 12 based on color\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, number_of_steps))\n",
    "    for i in range(0, number_of_steps):\n",
    "        plt.plot(result_df.loc[result_df['prediction']==predicted_column,i][:], label=f'predicted_{predicted_column}_{i}', color=colors[i], alpha=0.2)\n",
    "    # i =0\n",
    "    # plt.plot(df_shift[i], label=f'{i}_step_ahead')\n",
    "    plt.plot(result_df.loc[result_df['prediction'] == predicted_column,'truth'][:], label='truth')\n",
    "    plt.legend()   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISUUZtfMjacz"
   },
   "source": [
    "### Need to remove normalization\n",
    "- since we preformed normlization transform on 7 cols of dataframe, we need to add dummy colums to preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsWb1_Hojac3"
   },
   "source": [
    "### Get R2 for this prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot y_truth and y_pred\n",
    "plt.plot(y_truth[0], color = 'red', label = 'Real data')\n",
    "\n",
    "plt.plot(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lp5fVzFXjac4",
    "outputId": "3dfd642c-068c-44c2-cb44-baa29681ff50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "dRn6j-5Vjac9",
    "outputId": "973429fa-d13b-4616-c6bc-dd4c8062bfad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "gi4-BFd4jadC",
    "outputId": "a6b08730-8e2c-40f3-a222-7e591ef516e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(y_truth, color = 'red', linestyle='dotted')\n",
    "plt.plot(y_pred, color = 'blue', linestyle='dotted')\n",
    "# change x ticks to dates\n",
    "print(np.arange(0, df_test.shape[0], step=24))\n",
    "# create list of dates by 1 hour from 2023-11-01 00:00:00 to 2023-11-30 23:00:00\n",
    "start = pd.to_datetime('2023-12-16 19:00:00')\n",
    "end = start + pd.Timedelta(hours=df_test.shape[0])\n",
    "dates = pd.date_range(start, end, freq='1H')\n",
    "\n",
    "# transform dates to format MM-DD HH\n",
    "dates = dates.strftime('%Y-%m-%d')\n",
    "# set xticks to dates\n",
    "plt.xticks(np.arange(0, df_test.shape[0], step=24), dates[np.arange(0, df_test.shape[0], step=24)], rotation=45)\n",
    "\n",
    "#add legend\n",
    "plt.legend(['skutečnost','predikce'])\n",
    "\n",
    "# add x and y label\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Spotřeba tepla [kJ]')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0h37CyIjadH"
   },
   "source": [
    "## Let's try the evaluate_generator in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pQsfYFgcjadH",
    "outputId": "6bcc04f0-c1f7-4dd5-ce41-bd69f9534416"
   },
   "outputs": [],
   "source": [
    "(_,r2) = model.evaluate_generator(generator=test_gen,steps=1,workers=1)\n",
    "print('R2 = ',r2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_test_with_multivariate_time_series.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
